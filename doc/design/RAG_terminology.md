## RAG 相關術語解釋

本文檔旨在記錄和解釋在 AI 問答系統架構中遇到的關鍵術語，以便團隊成員有共同的理解。

---

### 1. SSE (Server-Sent Events)

**SSE (Server-Sent Events)** 是一種網頁技術，允許伺服器在與客戶端（瀏覽器）建立一次性連線後，能夠**主動、單向地**持續向客戶端推送更新資料。

**核心概念：**

-   **單向串流**：資料流動方向為 `伺服器 -> 客戶端`。
-   **持久連線**：客戶端發起請求後，伺服器會保持該 HTTP 連線開啟，以便隨時發送事件。
-   **自動重連**：如果連線中斷，瀏覽器會自動嘗試重新連線。
-   **輕量高效**：相較於 WebSocket，SSE 基於標準的 HTTP/HTTPS 協議，實作更為簡單，且資源消耗較低。

**在履歷問答專案中的應用：**

主要用於實現 AI 回答時的「打字機效果」。當後端語言模型（LLM）逐字生成答案時，會透過 SSE 將每一個生成的小片段（token）即時串流到前端。

-   **優點**：
    -   **改善使用者體驗**：使用者能立刻看到回應，減少等待的焦慮感。
    -   **降低首次回應時間 (TTFB)**：讓系統感覺反應更為靈敏。
-   **與 WebSocket 的比較**：WebSocket 提供雙向溝通，功能更強大但實作稍重；對於僅需伺服器推送更新的場景（如本專案），SSE 是更合適的選擇。

---

### 2. Ingestion (資料擷取與處理)

在 RAG (Retrieval-Augmented Generation) 系統中，**Ingestion** 指的是一套完整的**「資料擷取、處理、轉換並載入」**的流程。這是建構任何檢索系統的基礎準備工作。

**核心目標：**

將原始、非結構化的資料（例如 `resume.json` 或 `.tex` 文件），轉換為可供 AI 高效檢索的格式（通常是向量），並存入專門的資料庫（向量資料庫）中。

**主要步驟：**

1.  **讀取 (Load)**：從來源讀取原始資料。
2.  **切分 (Chunking / Split)**：將長篇的資料策略性地拆分成有意義、固定大小的文字區塊 (chunks)。精準的切分策略是提升檢索品質的關鍵。
3.  **向量化 (Embedding)**：使用嵌入模型（如 `text-embedding-3-small`）將每一個文字 chunk 轉換為一個數學向量 (vector)。這個向量代表了該段文字的語義。
4.  **儲存 (Store)**：將處理好的 chunks、中繼資料 (metadata) 及其對應的向量，一同存入向量資料庫（如 Supabase/pgvector）並建立索引。

**在履歷問答專案中的應用：**

整個流程是為了將 `resume.json` 的內容「預處理」成一個 AI 能夠理解並能快速查找的知識庫。`POST /rag/ingest` 這個 API 的作用就是觸發並執行此流程。一旦 Ingestion 完成，系統就能在使用者提問時，透過語義相似度快速找到最相關的履歷內容，以生成精準的回答。

---

### 3. Embedding (向量化) 與模型選擇

**Embedding (或稱「嵌入」)** 是將離散的資料（如文字）轉換為連續、稠密的數字向量的過程。這個向量能夠捕捉資料的語義資訊，使得電腦能夠理解文字之間的關係。在 RAG 系統中，它被用來將使用者的問題和知識庫中的文件轉換到同一個語義空間，以便進行相似度比較與檢索。

#### OpenAI 主要 Embedding 模型比較

以下是 OpenAI 提供的三款主流模型的比較表，以協助選擇最適合的方案：

| 特性 | **text-embedding-3-small** | **text-embedding-3-large** | **text-embedding-ada-02** |
| :--- | :--- | :--- | :--- |
| **價格 (每 1K tokens)** | **$0.00002** (最便宜) | $0.00013 | $0.00010 |
| **最大輸入 (tokens)** | 8191 | 8191 | 8191 |
| **輸出向量維度** | **1536** | 3072 | 1536 |
| **MTEB 效能分數**¹ | **62.3%** | **64.6%** (最高) | 61.0% |
| **MIRACL 效能分數**² | **44.0%** (多語言佳) | **54.9%** (多語言最強) | 31.4% |
| **發布時間** | 2024 年 1 月 | 2024 年 1 月 | 2022 年 12 月 |

> ¹ **MTEB (Massive Text Embedding Benchmark)**: 衡量模型在多種**英文**任務上的綜合表現。
> ² **MIRACL (Multilingual Information Retrieval Across a Continuum of Languages)**: 衡量模型在**多語言**檢索任務上的表現。

#### 各模型分析與選擇建議

1.  **`text-embedding-ada-002`**
    *   **分析**：過去兩年的標準模型，但目前在價格和效能上均已被新模型超越。
    *   **結論**：**不建議**在新專案中使用。

2.  **`text-embedding-3-large`**
    *   **分析**：OpenAI 目前**最強大**的 embedding 模型，效能頂尖。但價格是 `small` 版本的 6.5 倍，且輸出的 3072 維向量會佔用雙倍的儲存空間。
    *   **結論**：適合追求極致效能且預算充足的應用，但對履歷問答專案而言**效能過剩（Overkill）**。

3.  **`text-embedding-3-small`**
    *   **分析**：新一代的**高性價比模型**，成本極低，同時效能（特別是多語言）全面超越 `ada-002`。
    *   **結論**：**強烈推薦**。它完美地平衡了成本與效能，是當前絕大多數應用場景（包含本履歷問答系統）的最佳選擇。

#### 專案最終建議

針對本專案處理**使用者簡短提問**與**結構化的中英履歷資料**的情境，**`text-embedding-3-small` 是最適合的模型**。它能以最低的成本，提供完全足夠且優異的語義檢索能力。

---

#### Embedding 實務策略與原因

在將資料轉換為向量的過程中，除了選擇合適的模型，還需要搭配一系列工程策略，以確保整個流程的效率、穩定性與成本效益。

1.  **批次處理 (Batching)**
    *   **原因**：
        *   **提升吞吐量 (Throughput)**：與其單獨為每個文字區塊 (chunk) 發送一次 API 請求，不如將數十甚至上百個 chunks 組成一個批次 (batch) 一次性發送。這能大幅減少網路往返的次數，顯著提升整體處理效率。
        *   **降低延遲**：對於整個資料導入 (Ingestion) 過程來說，批次處理能大幅縮短總耗時。

2.  **退避重試 (Exponential Backoff and Retry)**
    *   **原因**：
        *   **處理網路不穩定與 API 限制**：任何網路服務（包含 OpenAI API）都可能因暫時性問題（如網路抖動、伺服器過載）而請求失敗，或觸發速率限制 (Rate Limit Exceeded)。
        *   **增強系統強韌性 (Resilience)**：加入「退避重試」機制後，系統會在請求失敗時自動等待一段時間（等待時間會以指數級增長，此即「退避」），然後再次嘗試。這能優雅地處理暫時性錯誤，確保流程的穩定性。

3.  **快取重用 (Caching and Reuse)**
    *   **原因**：
        *   **節省成本**：Embedding API 按 token 使用量收費。對於內容不變的文字 chunk，每次都重新計算 embedding 是一種浪費。透過快取機制，若遇到完全相同的文字內容，可直接從快取中讀取已計算好的向量，無需再次呼叫 API，直接節省成本。
        *   **加快執行速度**：從本地快取（或 Redis 等快取服務）讀取資料，遠比透過網路呼叫外部 API 更快。

4.  **輸入正規化 (Input Normalization)**
    *   **原因**：
        *   **提升向量品質**：Embedding 模型會將文字的「語義」轉換成數學向量。如果輸入的文字中含有不必要的雜訊（如多餘的空白、換行符、特殊字元），這些雜訊也會干擾向量的計算結果，導致語義表達不夠精準。
        *   **確保語義一致性**：`" 這是一段經歷 "` 和 `"這是一段經歷"` 在語義上完全相同，但因前者多了空白，產生的向量可能會有些微差異。正規化（例如去除頭尾空白）可以確保相同語義的內容產生更一致的向量，進而提升後續向量搜尋的準確度。

**總結**：這些策略的組合是確保資料向量化階段能夠做到**快、穩、省、準**的關鍵工程實踐。

---

- **是否需要 REST？** 在此情境通常使用「REST + SSE」：
  - **優點**：相容現有前後端架構與安全設計（API Gateway、WAF、Cache）、易測（curl/Postman）、可觀測性佳（結合 Langfuse/日誌）。
  - **缺點**：非雙向；若需即時指令（tool calls 主動推播）或協作式編輯，WebSocket 更合適。JSON 體積也大於 gRPC。
  - **常見做法**：Web 端聊天多採用 REST + SSE；服務間若需求嚴謹型別與效能，會引入 gRPC。